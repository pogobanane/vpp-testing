\chapter{Evaluation and Model}

% show measurement results and explain that headers are parsed for xconnect/bridging even if they are not needed \ref{}

% mention the missing mac addr setting in vpp16.09

% talk about max fib sizes

% mention that for fibs only one via target is used as in rfc defined

% how well does the fantastic memory alignment of vpp work in numa?


\section{CPU as Bottleneck}
\label{sec:cpubottleneck}

Table \ref{bottleneck} presents maximum throughput of all
measured \Ac{vpp} configurations with minimum sized packets using a single \Ac{vpp}
worker. 

The results show, that the computationally least expensive scenario
(xconnect) allows for the highest packet rates. Furthermore we see,
that when reducing the CPU clock speed by 3\%, the number of processed
packets shrinks by around 3\%, too. This indicates, that the CPU
cycles are bottlenecking the packet throughput of \Ac{vpp}.

This means the packet throughput $f(c)$ can be approximated depending on
the clock speed $c$ for maximum packet rates $p_{max}$ and the maximum
clock speed $c_{max}$:

% f(50%) = 55%
% f(97%) = 98%
% f(100%)= 100%
% f(x) = 0.9 * x + 10

$$ f(c) = (0.9 * \frac{c}{c_{max}} + 0.1) * p_{max} $$

This models approximates for 50\% of the maximum clock speed a
throughput of 55\% of the maximum one. This does not exactly resemble
the expected function $f(c) = \frac{c}{c_{max}} * p_{max}$ (half
the performance at half the clock speed) which were to be expected if
the CPU cycles were the only limiting factor. This expected behavior
can be observed though with MoonRoute \cite{chair:architecture} which
indicates another limiting factor, besides CPU
frequency.

% could it be, that vectorization or multi loop gives a base performance boost?

% TODO: automate tests for this table and look at i/d cache misses and perf records

\begin{table}[!ht]
	\vspace{5ex}
	\begin{tabular}[]{ l r r r }
		Scenario & 1.6GHz (50\%) & 3.1GHz (97\%)  & 3.2GHz (100\%) \\ 
		\midrule
		xconnect & 7.34 (56\%) & 12.90 (98\%) & 13.20 (100\%) \\ % stdDev 0.03 0.02 0.12
		l2 bridge no features & 6.69 (55\%) & 11.84 (97\%) & 12.18 (100\%) \\ % 0.01 0.03 0.08
		IPv4 1 route & 6.03 (53\%) & 10.98 (97\%) & 11.28 (100\%) \\ % stdDev 0.01 0.02 0.05
		l2 bridge mac-learn, mac-age & 6.05 (55\%) & 10.84 (98\%) & 11.09 (100\%) \\ % 0.01 0.05 0.02
		IPv6 1 route & 5.38 (53\%) & 9.87 (97\%) & 10.14 (100\%) \\ % 0.01 0.02 0.04
		VXLAN encap & 4.48 (55\%) & 8.15 (99\%) & 8.21 (100\%) \\ % 0.01 0.03 0.39
		IPv4 255k routes & 4.19 (58\%) & 7.06 (97\%) & 7.25 (100\%) \\ % 0.01 0.04 0.07
		IPv6 255k routes & 2.34 (62\%) & 3.72 (98\%) & 3.80 (100\%) \\ % 0.01 0.04 0.02
		\midrule
	\end{tabular}
	\caption{Maximum throughput (Mpps) in different scenarios with different CPU frequencies measured on the Xeon E3-1230 system. }
	\label{bottleneck}
\end{table}
% With offered Mpps beeing 100\%, "Relative" is the maximum packet throughput in relation to the offered one.


\section{CPU Scaling Model}

Since the CPU is a bottleneck in all tested configurations, the
ability to distribute the load over multiple CPU cores is
essential. Not all \Ac{vpp} configurations support this though.

\Ac{vpp} has two concepts for using multiple cores: It has workers for
simultaneous receiving and processing of packets over the processing
graph - and it can be configured to use another set of workers for
prioritized sending of packets (\Ac{hqos} \cite{vppdocs:qos}
\cite{vppdocs:hqosplacement}). Only the first type of worker can be
used to enhance the performance of raw processing throughput though.

Many layer 3 processing nodes such as the \Ac{ip4} and the \Ac{ip6}
processing path do support parallel packet processing using \Ac{rss}
(see Section \ref{sec:rss}). For the scaling to have effect, the
packets need different packet headers though, so that \Ac{rss} header
field hashing can assign them to different queues. Using four
different source addresses per queue is sufficient to hit all of them
well.

Layer 2 bridges on the other hand are completely single threaded.
Because \Ac{vxlan} needs a bridge to receive or send layer 2 traffic,
it can't utilize multiple queues, too.

Scaling measurements are conducted with a 10GbE and a 40GbE link with
high and low CPU clock speed. Tests begin with \Ac{vpp} in
single threaded mode, having main core and worker in the same thread
and continue with using unused physical cores for workers.
As Figure \ref{graph:multicore} shows, switching from single threaded
mode to a dedicated worker increases throughput slightly, but barely
noticeable. Two cores are already enough to saturate the 10GbE link.
The virtual ("hyper threading") cores are used for workers three to
six within the 10GbE system which results in lower performance gain
and even slight performance loss in the end.

Using the 40GbE system at maximum CPU clock two cores can just not
saturate the limits of the \Ac{nic}. Both 40GbE scaling tests show
that for a worker's maximum throughput $w_{t}$ the overall maximum
throughput depending on the number of workers $w$ is $f(w) = w *
w_{t}$. This holds true until line rate is hit at around two and four
workers, respectively (using physical cores).

At around 20 Mpps the measured throughput can be unstable by around 0.4
Mpps, because of unstable MoonGen packet generation rates. This is
due to the \Ac{nic} performance limit described in Section
\ref{sec:40gbelimit}.

% NAY what happens with different flows per core? low flows but high core count?

% simple cpu scaling on klaipeda 10GbE and omanyte 40GbE
% maybe show both with high and low cpufreq
% klaipeda easily saturated with 2 links
% omanyte with lower freq shows good scaling up to 4 cores

% singlethreaded is typically a ~x% slower than main thread and a single worker

% TODO model: cpu bottleneck model does not apply to omastar cpu. adjust? expand model for workers

\begin{figure}[!ht]
\noindent\hspace{0.5mm}\includegraphics[width=\linewidth]{pics/throughput_summary_multicore.pdf}
\caption{VPP CPU scaling with \Ac{ip4} traffic with 10GbE, 40GbE and different CPU clock speeds. All workers (besides E3-1230 core 3-6) use physical CPU cores. The 40GbE NIC bottlenecks at around 20Mpps (see Section \ref{sec:40gbelimit}). }
\label{graph:multicore}
\end{figure}


\section{Lookup Performance Model}

% present measurement results_
% - l2fib
% - ip4
% - ip6
% - v16.09 ip4


\subsection{L2 \Ac{fib}}

The layer 2 \Ac{fib} is tested by configuring \Ac{vpp} with varying
amounts of l2fib entries and sending packets to random destinations
using a single worker.

Figure \ref{graph:l2fib} shows that with only one l2fib entry you get
0.5 Mpps more throughput compared with two l2fib entries. This is due
to the simple caching mechanism of the l2fib-lookup function: It
stores the last looked up information locally, so that when the next
packet goes to the same destination, the local values can be used and
a hash table lookup can be skipped.

From there on there are three key points when the CPU's L1, L2 and L3
cache are full. We can model the BiHash l2fib lookup table. It takes
64 bit values as input and returns a 64 bit result. Assuming the table
only contains keys and values, the maximum \Ac{fib} size fitting into
cache of $n$ bytes is:

$$ f_{l2fib}(n) = n * \frac{8}{2 * 64} = n * \frac{1}{16}  $$

This formula is used to create the l1, l2 and l3 cache marks in Figure
\ref{graph:l2fib} as a upper bound for how many fib entries could fit
into the respective caches. As Figure \ref{graph:l2fib} shows, the throughput drops before the L1
cache as the L1 cache misses start rising. With the Layer 2 cache mark
the throughput drops further and more CPU time is spent inside the
\lstinline|l2fwd_node| function which does the lookup. Finally before
the layer 3 mark, the respective data cache misses rise, the
throughput drops further and close to 40\% of the CPU time is spent
inside the lookup node.


\begin{figure}[!ht]
\noindent\hspace{0.5mm}\includegraphics[width=\linewidth]{pics/throughput_l2_throughmac_klaipeda32ghz_v3.pdf}
\caption{Testing \Ac{vpp} v18.10 with different layer 2 \Ac{fib} sizes. Throughput, Layer 1 data cache load misses (divided by 10) and Last Level Cache load misses per packet and the percentage of CPU time spent in selected functions. }
\label{graph:l2fib}
\end{figure}

\newpage

\subsection{\Ac{ip6} \Ac{fib}}

Figure \ref{graph:ip6fib} shows the results of \Ac{ip6} routing with
different layer 3 \Ac{fib} sizes. Its maximum size is remarkably
smaller compared to the layer 2 \Ac{fib}. Each lookup table result
value is only 32 bit in size. Assuming the table only contains those
values and keys the size of an \Ac{ip6} address, the maximum \Ac{fib}
size fitting into cache of $n$ bytes is:

$$ f_{ip6fib}(n) = n * \frac{8}{128 + 32} = n * \frac{1}{24}  $$

This means for a layer 3 cache size of 8MB ($2^{23}$B) at most 350,000
could fit into it. At this \Ac{fib} size we are off the charts though
and there is no correlating change in throughput. Therefore the model
for layer 2 \Ac{fib}s seem not to apply to \Ac{ip6} \Ac{fib}s.

% TODO there is a change before 4096 in omastar 

% l2 cache size of 256k (2^18) at most 32,768 routes -> right after the big drop

Nevertheless it can be assumed the massive drop in performance from
20,000 \Ac{fib} entries upwards is because of the BiHash table lookups
taking longer because of cache size limitations, since the big drop of
throughput closely correlates to the layer 3 cache misses and the time
spent in the lookup function.



\begin{figure}[!ht]
\noindent\hspace{0.5mm}\includegraphics[width=\linewidth]{pics/throughput_l3v6_routes_klaipeda32ghz_v3.pdf}
\caption{Testing \Ac{vpp} v18.10 with different \Ac{ip6} \Ac{fib} sizes. Throughput, Layer 1 data cache load misses (divided by 10) and Last Level Cache load misses per packet and the percentage of CPU time spent in selected functions. }
\label{graph:ip6fib}
\end{figure}




\subsection{\Ac{ip4} \Ac{fib}}
\label{sec:ip4fib}

While the layer 2 \Ac{fib} can contain over $2^{23}$ entries, tests
show that \Ac{vpp} v18.10 stops working with more than 287,743
\Ac{ip4} \Ac{fib} entries. To be able to compare \Ac{vpp} to other
software routers nevertheless, tests conducted with v16.09 show that
this version is able to handle up to around 12,580,000 \Ac{fib}
entries which is well above $2^{23}$.

Figure \ref{graph:ip4fib} and \ref{graph:ip4fiblegacy} show test
results of \Ac{vpp} v18.10 with up to 255,000 entries and of v16.09
with up to 10,300,000 entries. Both show similar behavior to the
\Ac{ip6} \Ac{fib} tests with one big throughput drop towards the end
of the graph.

% TODO point out, that index lookup is not the performance issue

\begin{figure}[!ht]
\noindent\hspace{0.5mm}\includegraphics[width=\linewidth]{pics/throughput_l3_routes_klaipeda32ghz_v3.pdf}
\caption{Testing \Ac{vpp} v18.10 with different \Ac{ip4} \Ac{fib} sizes. Throughput, Layer 1 data cache load misses (divided by 10) and Last Level Cache load misses per packet and the percentage of CPU time spent in selected functions.}
\label{graph:ip4fib}
\end{figure}

\begin{figure}[!ht]
\noindent\hspace{0.5mm}\includegraphics[width=\linewidth]{pics/throughput_l3_routes_klaipeda_v1609_32ghz_v3.pdf}
\caption{Testing \Ac{vpp} v16.09 with different \Ac{ip4} \Ac{fib} sizes. Throughput, Layer 1 data cache load misses (divided by 10) and Last Level Cache load misses per packet and the percentage of CPU time spent in selected functions. }
\label{graph:ip4fiblegacy}
\end{figure}

\subsection{Lookup Nodes as Bottleneck}

With big lookup tables, the throughput drops a lot with every kind of
lookup table. For every additional CPU cycle spent for the
lookup in relation to the available cycles, the throughput will drop
by the same ratio. Thus the maximum throughput $f$ can be modeled
depending on the relative time $d_{lookup}$ spent in the lookup node
function for a known maximum throughput $t_{max}$ with one or two
routes:

$$ f(x) = t_{max} * ( 1 - d_{lookup} ) $$

The measurements closely resemble this model. They proof that the
lookup is basically exclusively responsible for the performance
decrease of bigger \Ac{fib}s and thus a limiting factor for scenarios with
big routing tables.






% model the performance decrease according to causes

% cpu scaling


\section{Latencies}

\subsection{Average Latencies}

% dropped frames per throughput, latency: avg, 99.999 percentile per throughput

Measuring the latency should be done with reduced offered load,
because when offering the load at full link speed, the average latency
hits a worst case which is orders of magnitude higher than at lower
packet rates. Figure \ref{graph:latencyoverview} shows the average
latency and the packet loss at different packet rates relative to the
\Ac{ndr} for \Ac{ip4} routing with 1 and with 255,000 routes. It shows
that the latency is around $\SC{5}{\mu s}$ for low packet rates and
slowly increases towards reaching the maximum throughput. When this
maximum is reached, the packet drop rates immediately go up, because
not all packets can be forwarded. This is also exactly the point where
the latency explodes to around $\SC{600}{\mu s}$ because packet queues
fill up.

The qualitative latency behavior described in Figure
\ref{graph:latencyoverview} between 1 and 255k routes is similar. The
following function can approximate the average latency for different
packet rates $t \in (0, t_{max})$ for a known maximum packet
throughput $t_{max}$. 

$$ l(t) = 59 - 62 * (-\frac{t}{t_{max}}+1)^{\frac{t_{max}}{8t}} $$ 

% this is already said in Latency Distribution

% Generally speaking, an increase in latency with growing load is
% expected, because of \Ac{vpp}'s vectorization. As explained in section
% \ref{sec:vectorization}, the lower the packet rate, the earlier
% batches are closed and processed even if the batch is hardly filled.
% Therefore individual packets have to wait less time at average to be
% processed.

\begin{figure}[!ht]
\noindent\hspace{0.5mm}\includegraphics[width=\linewidth]{pics/latencies_per_throughput_summary_ip4.pdf}
\caption{Latencies in $\mu s$ and packet drops of \Ac{ip4} routing with a single route (1 r) and 255k routes (255k r). }
\label{graph:latencyoverview}
\end{figure}

\subsection{Latency Distribution}

Figure \ref{graph:latencyhistogram} shows histograms of latency for
\Ac{ip4} routing at approximately 10\%, 50\% and 90\% of the maximum
throughput rate. It shows that under very low load the latency
distribution peaks in the beginning. Later on, the distribution can be
approximated as a standard distribution but with growing average and
standard deviation. The reason for this is lies within \Ac{vpp}
closing the batches as long as the packet processing is not stressed
to its limits (see Section \ref{sec:vectorization}). This results in earlier
processing of packets at low loads and explains why at 50\% of the
maximum load the latency is the same, even though the lookup load is
different. \cite{linguaglossa2017high}


% histogram
% ip4 1route low, 90\% and max load (0.1, 0.5, 0.9)
% ip4 255routes low, 90\% and max load

% 1r 7.8mpps 50% 8502.523457,		 35% 992.510538			28% ixgbe_xmit_pkts 31736245724cycles
% 1r 10.86mpps 90% 25631.480224,	100% 2790.657240

% 255r 5.87mpps 50% 11283.178205,2072.334996		18% ixgbe_xmit_pkts 31488860702
% 255r 7.28mpps 90% 911243.365558,9960.445080

% serialization takes 30% of time -> ( cycles/s / packets/s ) * 0.3 * cycles/s => 0.3 * latency = 3ys tx time
% => tx time is approximately latency stdDev
% Each batch spends 30\% of its time in the tx node. This means the first packet will be sent approximately 0.3*latency before the last one. This is the latency. 
% As figure \ref{graph:latencyhistogram} shows, this holds true for high packet throughputs when batches are always nearly full. For very low throughput the histogram does not resemble the standarddeviation and indicates that many packets get their very own vector. Thus the big spike at the beginning of the distribution. 

% TODO: test traffic patterns. 
% TODO: check back l2 bridge histograms which were more interesting.

\begin{figure}[!ht]
\noindent\hspace{0.5mm}\includegraphics[width=\linewidth]{pics/latency_histogram_overview_ip4.pdf}
\caption{Latency histogram for \Ac{ip4} routing with a single route and 255k routes at approximately 10\%, 50\% and 90\% of the maximum throughput rate. }
\label{graph:latencyhistogram}
\end{figure}


% \subsection{Bottleneck Analysis}

% - fib: memory cache speed
% - cpu cycles
% - other hardware bottleneck? but this was already written about in analysis


\section{Comparison}

Tests showed that there are big performance differences even between
\Ac{vpp} versions. While v16.09 is slower when using little \Ac{ip4}
routes, it doesn't have the \Ac{fib} size limit of 255k which is
exceptionally low compared to other software routers. Generally
speaking though, it is at least twice as fast as Click DPDK and has
similar performance to FastClick DPDK. \Ac{vpp} v18.10 being 1.2 Mpps
faster than FastClick DPDK indicates better optimizations and higher
potential for \Ac{vpp} v18.10 - but for routing tables with 30,000 to
1,000,000 entries FastClick DPDK has a very clear lead.

% vectorization of vpp and fastclick and not the others

\begin{table}[!ht]
	\vspace{5ex}
	\begin{tabular}[]{ l r r r }
		Implementation	 & FIB sizes & Mpps		& Relative \\ 
		\midrule
		MoonRoute		 & 1		 & 14.6		& 100\% \\
		MoonRoute		 & $2^{20}$	 & 14.2		& 97\% \\
		MoonRoute		 & $2^{24}$	 & 11.6		& 79\% \\
		VPP v18.10		 & 1		 & 11.6		& 79\% \\
		FastClick DPDK	 & 1		 & 10.4 	& 72\% \\
		FastClick DPDK	 & $2^{20}$	 & 10.4 	& 72\% \\
		VPP v16.09		 & 1		 & 9.7	 	& 71\% \\
		VPP v16.09		 & 255k		 & 9.2	 	& 63\% \\
		VPP v16.09		 & $2^{20}$	 & 8.5	 	& 58\% \\
		VPP v18.10		 & 255k		 & 7.2	 	& 50\% \\
		VPP v16.09		 & $2^{23}$	 & 6.5	 	& 45\% \\
		Click DPDK		 & 1		 & 4.3 		& 29\% \\
		Click DPDK		 & $2^{20}$	 & 4.2 		& 28\% \\
		Linux 3.7		 & 1		 & 1.5 		& 11\% \\

		\midrule
	\end{tabular}
	\caption{Comparison of maximum \Ac{ip4} forwarding throughput with a single worker on the Xeon E3-1230 system (see Table \ref{table:hardware}). Non-VPP results are from \cite{chair:architecture} and are conducted on the same system. }
	\label{table:comparison}
\end{table}

% TODO: moonroute, fastclick and click need citation

% \subsubsection{TODO sections:}

% Compare own measurements with figure 5+6 regarding latencies (linux, microtik, freebsd) of revisiting benchmarking methodology for interconnected devices.
% + frame losses (fig. 3):