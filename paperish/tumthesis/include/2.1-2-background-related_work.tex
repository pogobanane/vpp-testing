
\section{Background}

\subsection{Core Features}

% features:
% - modularity, feature rich
% - vectorization
% - RSS
% - dpdk: fast userspace NIC drivers

Core features of \Ac{vpp} are... 

... it's vectorized processing of packets in badges as the name
indicates. This allows for better optimization.

... it's support for \Ac{dpdk} with it's fast user-space NIC drivers.
With it come also virtual packet interfaces for quick links to local
virtualized systems.

... it's modular packet processing graph. Specific features are
implemented in nodes for example like ip4-lookup, ip6-lookup or
vxlan4-encap and vxlan6-encap. New features can be added with plugins
extending the packet processing graph by new nodes, adding new
processing paths and adding new CLI commands.


\subsection{Releases}

% releases:

Each release (such as v18.10) contains the following pre-release
milestones:  With the first label "F0" the API is frozen and
development shall aim for stability. For the second label "RC1" only
bug fix changes are accepted and a first artifact is released. Then
the iteration repeats and a second "RC2" version is released. Those
pre- releases are marked with a tag containing the version number and
the iteration appendix (v18.10-rc1). The final release will have no
extra tags in their name. The git tag marks the official release, but
the version-specific branch can still receive occasional updates.
\cite{vppwiki:releases}

The oldest version of VPP available in the github repository is from
2016. Unless otherwise specified this performance analysis used the
v18.10 branch from 14th of december 2018\footnote{commit
a8e3001e68d8f5ea6cf526b131c92f5993597f81}.


\subsection{Use Cases}

% interesting scenarios / use cases: 

A developer once formulated the design goal of \Ac{vpp}: It shall
seemlessly integrate into existing \Ac{sdn} stacks and it shall
"become the foundation of the future cloud native services"
\cite{florincoras}. That is, because \Ac{vpp} can serve in quite some
different scenarios and can connect to different control plane
tooling:

\subsubsection{Data Plane}

% - routing the internet: >500.000 routes

The internet the BGP table size grew to 512,000 prefixes in 2014 and
it continues growing. As \Ac{vpp} aims to be used for routing tasks,
it has to perform well with big routing tables. \cite{bgphelp:size}

% - scale to multiple cpu's

For handling high loads \Ac{vpp} helps spreading the computation
intensive frame processing tasks to multiple cpu cores. Qosmos for
example uses \Ac{vpp} to implement deep packet inspection
\cite{qosmos}.

% - latency behaviour

Viosoft on the other hand uses \Ac{vpp} in the context of embedded
systems which might require stable or short latencies \cite{viosoft}.
Therefore latency behaviour represents an important aspect of
performance analysis, too.

% - TODO more complex tasks like vxlan encapsulation (only if i need more text)
% In \Ac{sdn} environments increasing virtualization of hardware also leads to a virtualization of physical links. P

\subsubsection{Control Plane Integration}

\Ac{vpp} integrates into open source control plane projects:
OpenDaylight software can do some VPP management using an OpenFlow
interface via \Ac{lisp} flow mapping \cite{opendaylight:lisp}. Using
control plane software like OpenDaylight, \Ac{vpp} also integrates
into OpenStack Neutron, a networking management software for the
OpenStack cloud computing platform \cite{fdio:integration}.

% vPE: 39 times in source code, 
% - Cisco Virtual Multiservice Data Center (VMDC)
% - vPE (Virtual Provider Edge) provides VMDC 1-4.x

Furthermore it can be assumed that Cisco's \Ac{vpe} product group uses
VPP, because the term "vPE" can be found 39 times in the source code
and many additional times in the official documentation
\cite{vppdocs}. \Ac{vpe} is a collection of Cisco's concepts for
\Ac{sdn}, virtualized datacenters and cloud native environments called
\Ac{vmdc} 1.x to 4.x. Those \Ac{vpe} concepts are used in several
Cisco products \cite{cisco:sdn}:

\begin{itemize}
	\item Cisco VSG (Virtual Security Gateway)
	\item Cisco ASA 1000v, a Cloud Firewall
	\item Cisco vWAAS (Virtual Wide Area Applications Services)
	\item Citrix ADC VPX, a SDN suite
\end{itemize}


\section{Related Work}

% \cite{openvswitch:gym} u sure u wanna promote this?

There has been a previous work analyzing and describing the design,
architechture and performance of \Ac{vpp} \cite{linguaglossa2017high}.
It presents measurement results mainly regarding very specific
properties like packet vector size or other optimization strategies
but lacks results about the impact of hash table sizes. This paper on
the other hand presents detailed test results for the layer 2
\Ac{fib}, \Ac{ip4} \Ac{fib} and \Ac{ip6} \Ac{fib} which allow for a
quantitative performance comparison to other software router in a
routing scenario.

% differences: i haz:
% - l2fib, ip4/6 l3fib routing table entries
% - high core count cpu scaling
% - models describing performance behaviour
% - more in-depth latency description and model
% - more independenet

For \cite{linguaglossa2017high} the vpp nodes were extended to collect
stats, such as \Ac{perf} does, for all processing functions
individually which gives further insights. This paper on the other
hand (see chapter \ref{sec:methodology}) uses perf-record and perf-
report, to compare used cpu time off \Ac{vpp} functions.

% i really dont wanna admit this:

% There is also a repository \cite{github:vpp-bench} containing some
% \Ac{vpp} startup and setup scripts used for
% \cite{linguaglossa2017high}.

There is a paper \cite{revisiting-benchmarking:1} which tries to do
RFC2544-compliant benchmarks with FreeBSD, Linux and an off-the-shelf
MicroTik router. It summarizes existing methodology and benchmarking
systems and presents relevant parts of the results.

Another paper \cite{chair:architecture} presents benchmarking results
about the MoonRoute software router and some comparative data to
FastClick DPDK, Click DPDK and Linux 3.7.

There is also detailed analysis of the DPDK framework, comparison to
other frameworks and a model describing it's performance. \cite
{compare-highperf}

% rfc2544 not usable

Unfortunately there is no universally fitting and established
benchmarking routine. Even though RFC2544 \cite{rfc2544} describes
"Benchmarking Methodology for Network Interconnect Devices", many of
the suggestions done in it are not relevant or applicable to software
routers like \Ac{vpp}. \cite{revisiting-benchmarking:1}

RFC bs begin: % TODO

% frame sizes

For example it stresses procedures to test different ethernet frame
sizes. Frame size is no limiting factor inside the \Ac{dut} though (in
software routers with high packet rates) and is thus not necessary for
upper bound performance analysis. \cite{emmerich2015assessing}

% - "At the start of each trial a routing update
%    MUST be sent to the DUT." (long running, no good idea)

Furthermore the RFC states: "At the start of each trial a routing
update MUST be sent to the DUT" \cite{rfc2544}. Adding up to several
million routing entries using networking protocols before every test
run doesn't appear to be a feasable approach. Instead \Ac{vpp}'s CLI
is used to add all table entries using a single command.

% - each test-run SHOULD take over a minute. (no change after 20
%   seconds with turbo boost disabled) latency measurement one packet
% - per minute - to slow rate

Generally speaking, the RFC's requirements torwards the duration of
test runs exceeds what was done for this paper. According to the RFC a
test run should take longer than a minute - latency measurements shall
only measure timings of a single packet per minute. Providing a
constant level of CPU performance for example by disabling Intels
turbo-boost, \Ac{vpp} stabilizes after only 10 to 20 seconds of
receiving load which allows for way shorter test runs.

RFC bs end % TODO

% this one uses definitions of rfc1242. 
% usable: throughput and latency

RFC 2544 uses some definitions from RFC1242 \cite{rfc1242} like
"Throughput": RFC 1242 defines throughput as the highest throughput
without any packet loss, which is not the same as the maximum
measurable throughput. This paper refferes to throughput as the
maximum measurable throughput if not specified otherwise.

% - latency measurement at the rfc1242 throughput rate -> doesnt work
% because of variance

% MoonGen:

% this replaces the commented paragraph, because that was too
% complicated to get correct.

For deeper insights into how the throughput and latency is measured by
MoonGen, a tool used for this paper, \cite{emmerich2015moongen}
gives a lot of insight for example about the time measurement
accuracy. 

% "moongen latency measurement is science stadtard" - paul
% RFC 1242's definition of latency conforms to the latency measured by
% MoonGen which was used for this paper. RFC 2544 on the other hand
% additionally requires to measure the latency at the throughput
% according to RFC 1242. As this paper will show, the latency at very
% high throughput rates without packet loss varies a lot. Instead in
% order to get comparable latency results, a lower packet rate has to be
% selected.

% TODO (if more text needed) - special frames: broadcast, management, routing update
% mentioned in rfc, but not looked at in this paper

% TODO: add info about additional testing methodology like ip6 https://www.rfc-editor.org/rfc/rfc5180.txt
% Additionally RFC 5180 proposes \Ac{ip6} benchmarking methodology. 